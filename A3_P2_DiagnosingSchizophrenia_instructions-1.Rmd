---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Matilde Jacobsen"
date: "October 17, 2017"
output: 
  md_document:
    variant: markdown_github
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 3 - Part 2 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia. We then looked at whether we could replicate results from the previous literature.
We now want to know whether we can automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.
Again, remember that the dataset containst 7 studies and 3 languages. Feel free to only include Danish (Study 1-4) if you feel that adds too much complexity.

Issues to be discussed your report:
- Should you run the analysis on all languages/studies at the same time? 
- Choose your best acoustic feature from part 1. How well can you diagnose schizophrenia just using it?
- Identify the best combination of acoustic features to diagnose schizophrenia using logistic regression.
- Discuss the "classification" process: which methods are you using? Which confounds should you be aware of? What are the strength and limitation of the analysis?
- Bonus question: Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them. 
- Bonus Bonus question: It is possible combine the output of multiple  classification models to improve classification accuracy. For inspiration see,
https://machinelearningmastery.com/machine-learning-ensembles-with-r/
 The interested reader might also want to look up 'The BigChaos Solution to the Netflix Grand Prize'

## Learning objectives
- Learn the basics of classification in a machine learning framework
- Design, fit and report logistic regressions
- Apply feature selection techniques

### Let's start

We first want to build a logistic regression to see whether you can diagnose schizophrenia from your best acoustic feature. Let's use the full dataset and calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve). You need to think carefully as to how we should (or not) use study and subject ID.

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures. Alternatively, the groupdata2 and cvms package created by Ludvig are an easy solution. 

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one dataset, so to calculate overall performance.
N.N.N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?
N.N.N.N.B. A more advanced solution could rely on the tidymodels set of packages (warning: Time-consuming to learn as the documentation is sparse, but totally worth it)


```{r}
# load
library(tidyverse,pacman)
pacman::p_load(purrr,janitor, lme4, parsnip, groupdata2, tidymodels, caret, e1071, pROC, cvms, kernlab, randomForest, xgboost)

# load csv
feature_df <- read.csv("acoustic_features_by_language.csv")

```

2. Find best feature from A3P1 (by AIC and BIC on training data)
a) by creating logistic regressions for each feature (all 4)
b) Think carefully as to how we should (or not) use study and subject ID
c) Then run ANOVA
```{r}
#creating logistic regressions for choosing best feature
model1 <- glmer(Diagnosis ~ PitchVariability + (1|uID), data = feature_df, family = "binomial")
summary(model1)

model2 <- glmer(Diagnosis ~ PhonatationTime + (1|uID), data = feature_df, family = "binomial")
summary(model2)

model3 <- glmer(Diagnosis ~ Speechrate + (1|uID), data = feature_df, family = "binomial")
summary(model3)

model4 <- glmer(Diagnosis ~ PauseDuration + (1|uID), data = feature_df, family = "binomial")
summary(model4)

#sammenlign
anova(model1, model2, model3, model4)
```

3. assess accuracy, sensitivity, PPV, NPV and ROC curves, when predicting on all the data the model has been trained on
```{r}
#add 1000 to uID
feature_df$uID <- feature_df$uID + 1000

simple_model <- glm(Diagnosis ~ PhonatationTime, data = feature_df, family = "binomial")

predicted_values <- predict(simple_model, feature_df, allow.new.levels = T)
pred_df <- tibble(predictions = predicted_values, actual = feature_df$Diagnosis) 
pred_df$predictions = ifelse(pred_df$predictions < 0.0, "0", "1") 
pred_df$predictions <- as_factor(pred_df$predictions)
pred_df$actual <- as_factor(pred_df$actual)#make sure it is a factor
caret::confusionMatrix(pred_df$predictions, pred_df$actual, positive ="0")

predicted_values2 <- predict(model2, feature_df, allow.new.levels = T)
pred_df2 <- tibble(predictions = predicted_values2, actual = feature_df$Diagnosis) 
pred_df2$predictions = ifelse(pred_df2$predictions < 0.0, "0", "1") 
pred_df2$predictions <- as_factor(pred_df2$predictions)
pred_df2$actual <- as_factor(pred_df2$actual)#make sure it is a factor
caret::confusionMatrix(pred_df2$predictions, pred_df2$actual, positive ="0")


plot(roc(pred_df2$predictions,feature_df$Diagnosis))

```

4. partition (groupdata2)
a) remember to take ID into account, gender and diagnosis should be equally distributed across folds
b) find out whether we should take study into account also
```{r}

featurefold <- groupdata2::fold(feature_df, k = 5, id_col = "uID", cat_col = c("Diagnosis", "Gender", "Study")) %>% arrange(.folds)

```

5. Then cross-validate the logistic regression and re-calculate performance on the testing folds (cvms)
```{r}

CV1 <- cvms::cross_validate(featurefold, "Diagnosis ~ PhonatationTime + (1|uID)", 
                      fold_cols = '.folds',
                      family = "binomial",
                      control = glmerControl(
                        optimizer = "nloptwrap",
                        calc.derivs = F,
                        optCtrl=list(xtol_abs=1e-8, ftol_abs=1e-8, maxfun = 1000)),
                      REML = FALSE,
                      rm_nc = FALSE
                      )
head(CV1)
cv_plot(CV1, type ='ROC')
```

6. The best combination of features?
a) test multiple models, and find accuracy, sensitivity, specificity for all models
b) discuss above parameters
c) choose best model(s)
```{r}
s
CV2 <- cvms::cross_validate(featurefold, "Diagnosis ~ PhonatationTime + PitchVariability + (1|uID)", 
                       fold_cols = '.folds',
                      family = "binomial",
                      control = glmerControl(
                        optimizer = "nloptwrap",
                        calc.derivs = F,
                        optCtrl=list(xtol_abs=1e-8, ftol_abs=1e-8, maxfun = 1000)),
                      REML = FALSE,
                      rm_nc = FALSE
                      )

head(CV2)
# Plot results for binomial model
cv_plot(CV2, type ='ROC')
```

7. Try other classification methods (Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them.)
```{r}
#Load unscaled data
df <- read.csv("features_unscaled.csv") %>% 
  mutate_at(c("Diagnosis"), as.factor)

#Choosing the relevant features
df <- df %>% select('Gender','Diagnosis','Study','PhonatationTime','PitchVariability','Speechrate','PauseDuration','uID')

##Paritioning##
set.seed(5)
df_list <- partition(df, p = 0.2, cat_col = c("Diagnosis", "Gender", "Study"), id_col = "uID", list_out = T)
df_test = df_list[[1]]
df_train = df_list[[2]]

#Removing ID column
df_test <- df_test %>% 
  select(-uID)
df_train <- df_train %>% 
  select(-uID)

#create recipe
rec <- df_train %>% recipe(Diagnosis ~ .) %>% # defines the outcome
  step_center(all_numeric()) %>% # center numeric predictors
  step_scale(all_numeric()) %>% # scales numeric predictors
  step_corr(all_numeric()) %>% 
  prep(training = df_train)

train_baked <- juice(rec) # extract df_train from rec

rec #inspect rec

#Applying recepe to a test
test_baked <- rec %>% bake(df_test)

#Demonstration of different ways of doing the same process
juice(rec) 
rec %>% bake(df_train)

##Creating models##
#logistic regression
log_fit <- 
  logistic_reg() %>%
  set_mode("classification") %>% 
  set_engine("glm") %>%
  fit(Diagnosis ~ . , data = train_baked)

#support vector machine
svm_fit <-
  svm_rbf() %>%
  set_mode("classification") %>% 
  set_engine("kernlab") %>%
  fit(Diagnosis ~ . , data = train_baked)

#random forest
RanfomForest <-
  rand_forest() %>%
  set_mode("classification") %>% 
  set_engine("randomForest") %>%
  fit(Diagnosis ~ . , data = train_baked)

#boosted tree
BoostedTree <-
  boost_tree() %>%
  set_mode("classification") %>% 
  set_engine("xgboost") %>%
  fit(Diagnosis ~ . , data = train_baked)

##Applying models to test set##

#get multiple at once
test_results <- 
  test_baked %>% 
  select(Diagnosis) %>% 
  mutate(
    log_class = predict(log_fit, new_data = test_baked) %>% 
      pull(.pred_class), #predicting class
    log_prob  = predict(log_fit, new_data = test_baked, type = "prob") %>% 
      pull(.pred_1), #probability outcome
    svm_class = predict(svm_fit, new_data = test_baked) %>% 
      pull(.pred_class), 
    svm_prob  = predict(svm_fit, new_data = test_baked, type = "prob") %>% 
      pull(.pred_1),
    ranfor_class = predict(RanfomForest, new_data = test_baked) %>% 
      pull(.pred_class),
    ranfor_prob  = predict(RanfomForest, new_data = test_baked, type = "prob") %>% 
      pull(.pred_1),
    btree_class = predict(BoostedTree, new_data = test_baked) %>% 
      pull(.pred_class),
    btree_prob  = predict(BoostedTree, new_data = test_baked, type = "prob") %>% 
      pull(.pred_1)
  )
  
view(test_results)
###Performance

#Performance metrics
metrics(test_results, truth = Diagnosis, estimate = log_class) %>% view()
metrics(test_results, truth = Diagnosis, estimate = svm_class) %>% view() #best accuracy
metrics(test_results, truth = Diagnosis, estimate = ranfor_class) %>% view()
metrics(test_results, truth = Diagnosis, estimate = btree_class) %>% view()

#ROC curve
#plotting the roc curve:
test_results %>%
  roc_curve(truth = Diagnosis, log_prob) %>% 
  autoplot()
test_results %>%
  roc_curve(truth = Diagnosis, svm_prob) %>% 
  autoplot()
test_results %>%
  roc_curve(truth = Diagnosis, ranfor_prob) %>% 
  autoplot()
test_results %>%
  roc_curve(truth = Diagnosis, btree_prob) %>% 
  autoplot()

```




```{r}
 # Train / Test
df_par <- partition(df, p = 0.2, cat_col = "Diagnosis", id_col = "ID", list_out = F)
df_train <- subset(df_par,.partitions==1) 
df_test <- subset(df_par,.partitions==2) %>% select(-c(Study,ID,.partitions))

# Cross Validation
cross_val_tbl = group_vfold_cv(df_train, group = c(Diagnosis,ID), v = 10)
rsample::pretty.group_vfold_cv(cross_val_tbl)



```


Feature pre-processing
```{r}
# Preprocessing recipe
rec <- df_train %>% 
  recipe(Diagnosis ~ . ) %>% # defines the outcome 
  step_scale( all_numeric() ) %>% # scales numeric predictors
  step_center( all_numeric() ) %>% # center numeric predictors
  prep(training = df_train, retain = TRUE)

# Apply recipe
df_train_n <- juice(rec)  %>% select(-c(Study,ID,.partitions))
df_test_n <- bake(rec, new_data = df_test, all_predictors()) %>% select(-c(Study,ID,.partitions))

```

Fidding models
```{r}

LogisticRegression <-
  logistic_reg() %>%
  set_mode("classification") %>% 
  set_engine("glm") %>%
  fit(Diagnosis ~ . , data = df_train_n)

SVM <-
  svm_rbf() %>%
  set_mode("classification") %>% 
  set_engine("kernlab") %>%
  fit(Diagnosis ~ . , data = df_train_n)

RanfomForest <-
  rand_forest() %>%
  set_mode("classification") %>% 
  set_engine("randomForest") %>%
  fit(Diagnosis ~ . , data = df_train_n)

BoostedTree <-
  boost_tree() %>%
  set_mode("classification") %>% 
  set_engine("xgboost") %>%
  fit(Diagnosis ~ . , data = df_train_n)

```

Evaluate models on the test partition
```{r}
test_results <- 
  df_test %>%
  as_tibble() %>%
  mutate(
    log_class = predict(log_fit, new_data = df_test_n) %>%
      pull(.pred_class), 
    log_prob  = predict(log_fit, new_data = df_test_n, type = "prob") %>%
      pull(.pred_Schizophrenia),
    svm_class = predict(svm_fit, new_data = df_test_n) %>% 
      pull(.pred_class),
    svm_prob  = predict(svm_fit, new_data = df_test_n, type = "prob") %>%
      pull(.pred_Schizophrenia),[â€¦])
metrics(test_results, truth = Diagnosis, estimate = log_class) %>% knitr::kable()
metrics(test_results, truth = Diagnosis, estimate = svm_class) %>% knitr::kable()

```

